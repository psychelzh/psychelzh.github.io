[
  {
    "objectID": "posts/2024-01-10-connectome-predictive-modeling/index.html",
    "href": "posts/2024-01-10-connectome-predictive-modeling/index.html",
    "title": "R package for connectome predictive modeling (CPM)",
    "section": "",
    "text": "Since last year, I started to use connectome predictive modeling method to analyze my functional connectivity data, though proposed by my advisor. I found it is a very useful method to predict individual phenotypes from functional connectivity data. Since I am now a heavy R user, I would like to use R to do the analysis. I found NetworkToolbox on CRAN, but the codes there are not so efficient. So I decided to write my own R package for CPM. The package is still under development, but I have already uploaded it to GitHub. You can find it here. Though the performance cannot keep up with the MATLAB version, it is still much faster than the codes in NetworkToolbox. I will keep updating it. If you have any suggestions, please let me know. Thanks!"
  },
  {
    "objectID": "posts/2024-01-09-bench-matrix-stats/index.html",
    "href": "posts/2024-01-09-bench-matrix-stats/index.html",
    "title": "Benchmark Matrix Operations",
    "section": "",
    "text": "Recently, I am trying to speed up my connectome predictive modeling code. I found that the matrix operations are the bottleneck. Specifically, I need a faster version of scale(). Based on this blog, I decided to benchmark the matrix operations in different packages from fastverse.\n\nlibrary(collapse)\nrequireNamespace(\"bench\")\n\n\nRow Means\n\nwithr::local_seed(1)\nbench &lt;- bench::press(\n  nrow = c(10, 100, 1000),\n  ncol = c(100, 1000, 10000),\n  {\n    data &lt;- matrix(rnorm(nrow * ncol), nrow = nrow)\n    bench::mark(\n      collapse = collapse::fmean(data),\n      Rfast = Rfast::colmeans(data),\n      matrixStats = matrixStats::colMeans2(data),\n      base = .colMeans(data, nrow, ncol)\n    )\n  }\n)\n\n\nplot(bench)\n\n\n\n\n\n\n\nFigure 1: Benchmark of row means\n\n\n\n\n\n\n\nRow SDs\n\nwithr::local_seed(1)\nbench &lt;- bench::press(\n  nrow = c(10, 100, 1000),\n  ncol = c(100, 1000, 10000),\n  {\n    data &lt;- matrix(rnorm(nrow * ncol), nrow = nrow)\n    bench::mark(\n      collapse = collapse::fsd(data),\n      Rfast = Rfast::colVars(data, std = TRUE),\n      matrixStats = matrixStats::colSds(data)\n    )\n  }\n)\n\n\nplot(bench)\n\n\n\n\n\n\n\nFigure 2: Benchmark of row SDs\n\n\n\n\n\n\n\nRow-wise Operations\nUnfortunately, based on this issue, rowwise computations are not easy to be speeded in matrixStats. So further benchmarking will drop it.\n\nwithr::local_seed(1)\nbench &lt;- bench::press(\n  nrow = c(10, 100, 1000),\n  ncol = c(100, 1000, 10000),\n  {\n    data &lt;- matrix(rnorm(nrow * ncol), nrow = nrow)\n    vec &lt;- rnorm(ncol)\n    bench::mark(\n      collapse = data %r-% vec,\n      Rfast = Rfast::eachrow(data, vec, \"-\"),\n      base = data - rep(vec, each = nrow)\n    )\n  }\n)\n\n\nplot(bench)\n\n\n\n\n\n\n\nFigure 3: Benchmark of row-wise operations\n\n\n\n\n\n\n\nScale\n\nfscale_rfast &lt;- function(x) {\n  means &lt;- Rfast::colmeans(x)\n  sds &lt;- Rfast::colVars(x, std = TRUE)\n  Rfast::eachrow(\n    Rfast::eachrow(x, means, \"-\"),\n    sds, \"/\"\n  )\n}\nwithr::local_seed(1)\nbench &lt;- bench::press(\n  nrow = c(10, 100, 1000),\n  ncol = c(100, 1000, 10000),\n  {\n    data &lt;- matrix(rnorm(nrow * ncol), nrow = nrow)\n    bench::mark(\n      collapse = fscale(data),\n      Rfast = fscale_rfast(data),\n      base = scale(data),\n      check = FALSE # base scale will add attributes\n    )\n  }\n)\n\n\nplot(bench)\n\n\n\n\n\n\n\nFigure 4: Benchmark of scale\n\n\n\n\n\n\n\nConclusion\nFrom the above figures, we will find Rfast is the fastest package for matrix operations. Previously, I have used collapse package, which is actually fast enough. But now I will switch to Rfast."
  },
  {
    "objectID": "cv-cn.html",
    "href": "cv-cn.html",
    "title": "个人简历",
    "section": "",
    "text": "psychelzh@outlook.com"
  },
  {
    "objectID": "cv-cn.html#教育经历",
    "href": "cv-cn.html#教育经历",
    "title": "个人简历",
    "section": "教育经历",
    "text": "教育经历\n博士 – 认知神经科学  北京师范大学 2019年9月 — 至今\n\n博士论文：一般认知能力的结构及其脑网络基础\n预计提交日期：2024年6月\n\n硕士 – 认知神经科学  北京师范大学 2014年9月 — 2017年6月\n\n硕士论文：儿童执行功能的结构与发展研究\n毕业时被评为北京市优秀毕业生\n\n本科 – 统计学  北京师范大学 2009年9月 — 2013年6月\n\n毕业时成绩年级前5%\nGPA: 3.7/4.0"
  },
  {
    "objectID": "cv-cn.html#研究经历",
    "href": "cv-cn.html#研究经历",
    "title": "个人简历",
    "section": "研究经历",
    "text": "研究经历\n博士研究  北京师范大学 2019年9月 — 至今\n\n使用行为数据建模方法，如基于扩散漂移模型模型、生成模型等，优化认知任务指标计算\n使用结构方程模型探索一般认知能力及其认知结构\n基于磁共振成像技术，建立功能连接网络，结合连接组学预测模型（connectome-based predictive modeling）和脑网络机制探索一般认知能力的脑网络基础\n\n研究助理  北京师范大学 2017年9月 — June 2019\n\n参与北京市脑计划，收集和分析儿童青少年的认知能力与脑成像数据\n使用结构方程模型探索儿童认知能力的结构与发展"
  },
  {
    "objectID": "cv-cn.html#专业技能",
    "href": "cv-cn.html#专业技能",
    "title": "个人简历",
    "section": "专业技能",
    "text": "专业技能\n\n统计分析：熟练使用R、Python进行数据分析，熟悉线性混合效应模型、结构方程模型和贝叶斯统计建模等\n脑成像技术：熟悉磁共振成像技术，会独立完成数据预处理和分析"
  },
  {
    "objectID": "cv-cn.html#论文发表",
    "href": "cv-cn.html#论文发表",
    "title": "个人简历",
    "section": "论文发表",
    "text": "论文发表\nZhang, L., Feng, J., Xue, G*. (In preparation). The neural substrates and structure of general cognitive ability based on multiple cognitive tasks.\nZhang, L., Feng, J., Liu, C., Hu, H., Zhou, Y., Yang, G., Peng, X., Li, T., Chen, C., & Xue, G*. (2024). Improved estimation of general cognitive ability and its neural correlates with a large battery of cognitive tasks. Cerebral Cortex, 34(2), bhad510. https://doi.org/10.1093/cercor/bhad510 (IF: 3.7, SCI二区)\nSheng, J., Wang, S., Zhang, L., Liu, C., Shi, L., Zhou, Y., Hu, H., Chen, C., & Xue, G*. (2023). Intersubject similarity in neural representations underlies shared episodic memory content. Proceedings of the National Academy of Sciences, 120(35), e2308951120. https://doi.org/10.1073/pnas.2308951120 (IF: 11.1, SCI一区)\nFeng, J., Zhang, L., Chen, C., Sheng, J., Ye, Z., Feng, K., Liu, J., Cai, Y., Zhu, B., Yu, Z., Chen, C., Dong, Q., & Xue, G*. (2022). A cognitive neurogenetic approach to uncovering the structure of executive functions. Nature Communications, 13(1), 4588. https://doi.org/10.1038/s41467-022-32383-0 (IF: 16.6, SCI一区)\nSheng, J., Zhang, L., Liu, C., Liu, J., Feng, J., Zhou, Y., Hu, H., & Xue, G*. (2022). Higher-dimensional neural representations predict better episodic memory. Science Advances, 8(16), eabm3829. https://doi.org/10.1126/sciadv.abm3829 (IF: 13.6, SCI一区)"
  },
  {
    "objectID": "cv-cn.html#会议海报",
    "href": "cv-cn.html#会议海报",
    "title": "个人简历",
    "section": "会议海报",
    "text": "会议海报\nZhang, L., Xue, G. The neural substrates of general cognitive ability based on multiple cognitive tasks. Poster presented at the Annual Meeting of the Society for Neuroscience, November 2023, Washington, DC. USA.\n\n\nCreated based on tutorial of https://www.cynthiahqy.com/posts/cv-html-pdf/"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Email: psychelzh@outlook.com  Website: https://psychelzh.github.io/  URL: https://github.com/psychelzh"
  },
  {
    "objectID": "cv.html#education-and-training",
    "href": "cv.html#education-and-training",
    "title": "Curriculum Vitae",
    "section": "EDUCATION AND TRAINING",
    "text": "EDUCATION AND TRAINING\nDoctor of Philosophy – Cognitive Neuroscience  Beijing Normal University September 2019 — July 2024\n\nDissertation: Cognitive And Neural Mechanisms Of General Cognitive Abilities: Evidence From Psychometrics And Brain Networks\n\nResearch Assistant  Beijing Normal University August 2017 — August 2019\n\nCollected and analyzed data from a large-scale longitudinal study of children’s cognitive development\nUsed structural equation modeling to explore the structure of children’s cognitive abilities\n\nMaster of Science – Cognitive Neuroscience  Beijing Normal University September 2014 — July 2017\n\nThesis: The Structure and Development Trajectory of Children’s Executive Function\nGraduated as an outstanding graduate of Beijing\n\nBachelor of Science – Statistics  Beijing Normal University September 2009 — July 2013\n\nGraduated as top 5% of the class\nGPA: 3.7/4.0"
  },
  {
    "objectID": "cv.html#main-publications",
    "href": "cv.html#main-publications",
    "title": "Curriculum Vitae",
    "section": "MAIN PUBLICATIONS",
    "text": "MAIN PUBLICATIONS\nZhang, L., Feng, J., Liu, C., Hu, H., Zhou, Y., Yang, G., Peng, X., Li, T., Chen, C., & Xue, G. (2024). Improved estimation of general cognitive ability and its neural correlates with a large battery of cognitive tasks. Cerebral Cortex, 34(2), bhad510. https://doi.org/10.1093/cercor/bhad510\nSheng, J., Wang, S., Zhang, L., Liu, C., Shi, L., Zhou, Y., Hu, H., Chen, C., & Xue, G. (2023). Intersubject similarity in neural representations underlies shared episodic memory content. Proceedings of the National Academy of Sciences, 120(35), e2308951120. https://doi.org/10.1073/pnas.2308951120\nFeng, J., Zhang, L., Chen, C., Sheng, J., Ye, Z., Feng, K., Liu, J., Cai, Y., Zhu, B., Yu, Z., Chen, C., Dong, Q., & Xue, G. (2022). A cognitive neurogenetic approach to uncovering the structure of executive functions. Nature Communications, 13(1), 4588. https://doi.org/10.1038/s41467-022-32383-0\nSheng, J., Zhang, L., Liu, C., Liu, J., Feng, J., Zhou, Y., Hu, H., & Xue, G. (2022). Higher-dimensional neural representations predict better episodic memory. Science Advances, 8(16), eabm3829. https://doi.org/10.1126/sciadv.abm3829"
  },
  {
    "objectID": "cv.html#conference-presentations",
    "href": "cv.html#conference-presentations",
    "title": "Curriculum Vitae",
    "section": "CONFERENCE PRESENTATIONS",
    "text": "CONFERENCE PRESENTATIONS\nZhang, L., Xue, G. The neural substrates of general cognitive ability based on multiple cognitive tasks. Poster presented at the Annual Meeting of the Society for Neuroscience, November 2023, Washington, DC. USA.\n\n\nCreated based on tutorial of https://www.cynthiahqy.com/posts/cv-html-pdf/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, I am Liang, Zhang, a Ph.D just graduated from Beijing Normal University. This is my personal blog for my everyday thoughts and coding discoveries."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Liang’s Blogs",
    "section": "",
    "text": "Balance Time Consuming on Programming and Others\n\n\n\n\n\n\ntime-balance\n\n\n\n\n\n\n\n\n\nMar 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR metaprogramming\n\n\n\n\n\n\nmetaprogramming\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR package for connectome predictive modeling (CPM)\n\n\n\n\n\n\nR-package\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark Matrix Operations\n\n\n\n\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-03-31-time-balancing/index.html",
    "href": "posts/2024-03-31-time-balancing/index.html",
    "title": "Balance Time Consuming on Programming and Others",
    "section": "",
    "text": "Let’s begin with Donald Knuth’s famous quote:\n\nProgrammers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.\n\nI should admit that I have often found me into such a trap. To be more specific, I have spent much time on enhancing code quality and re-check it many times. Now if I am to be a researcher, how large proportion of my time should be spent on this sort of things?\nThere can be two extreme cases: hasty or meticulous. I should identify myself as the meticulous group. When working on projects, I will spend much of my time on programming and enhance the code quality. Sometimes it is definitely overemphasized, because I have postponed the deadline of many projects. It is of a merit to be meticulous, which is known to all, but that seems to be a problem for me and makes me not see the forest for the trees. This could really be an issue for a researcher, which is expected to focus more on information and knowledge.\nThe solution is to balance. To balance, I mean I should limit the time on programming. I love programming, but the more interesting, or important, things should be beyond programming but the knowledge of my research field.\nAt last, I am now working on my Ph.D dissertation, the programming hours should definitely limited. And so do the gaming and playing hours. Do not pretend to be diligent!"
  },
  {
    "objectID": "posts/2024-03-19-r-metaprogramming/index.html",
    "href": "posts/2024-03-19-r-metaprogramming/index.html",
    "title": "R metaprogramming",
    "section": "",
    "text": "Since I have learnt about the rlang package, I have been fascinated by the power of metaprogramming in R. The first time I learnt such a programming pattern might date back to when I found the eval() function in writing MATLAB codes. However, this type of programming is really a disaster in MATLAB, for it only accepts a string of code. It is R shows me the interesting part of the world of metaprogramming.\nAt that time, I knew it is called non-standard evaluation (NSE) in R, and it is just used to save typing so that users do not need to type quotes, and what’s more useful, I can access the data columns without typing the data variable names. Then after reading advanced R, it reveals a new path about metaprogramming in R, although some strange terms also make me confused, e.g., quasiquotation, unquotation, and so on.\nWhat makes me really write codes with metaprogramming paradigm is when I began to use targets package. And one day, I found “static branching” supported by the package, which requires certain metaprogramming. And the author of that package also created targetopia, really requiring some metaprogramming skills to create so-called targets factory functions. So I then resorted to rlang package, trying to have a deeper understanding of metaprogramming in R.\nThe first days with rlang package were really happy and funny. But the first frustration came when I found expr() is really annoying becasue I treated it as equivalent of substitute(), only also supports !! and !!! injection. After a real long time, I finally knew it is equivalent to bquote(). At the end of the day, I just begin to love base R, especially for metaprogramming. For example, bquote() and substitute() are more intuitive to me.\nBut today I got to know that bquote() is more power than I thought. It also supports splicing!! Amazing, although only added after R 4.0.0. I write a simple use case for me, which is really special for usage in targets package. As I sometimes nested name for several targets into a list, and I want to make sure targets understand that the dependency is the elements of the list, not the list itself. So the splicing feature of bquote() is really useful for me. Here is a simple example:\n\ntargets::tar_dir({\n  targets::tar_script({\n    library(targets)\n    deps &lt;- rlang::syms(c(\"x\", \"y\"))\n    list(\n      tar_target(x, 1),\n      tar_target(y, 2),\n      eval(\n        bquote(\n          tar_target(\n            name,\n            list(..(deps)) # splice syntax\n          ),\n          splice = TRUE\n        )\n      )\n    )\n  })\n  targets::tar_visnetwork()\n})"
  }
]